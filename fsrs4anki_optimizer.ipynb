{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FSRS4Anki v3.16.1 Optimizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lurCmW0Jqz3s"
      },
      "source": [
        "[![open in colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-spaced-repetition/fsrs4anki/blob/v3.16.1/fsrs4anki_optimizer.ipynb)\n",
        "\n",
        "â†‘ Click the above button to open the optimizer on Google Colab.\n",
        "\n",
        "> If you can't see the button and are located in the Chinese Mainland, please use a proxy or VPN."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7bBfGJFbMr"
      },
      "source": [
        "Upload your **Anki Deck Package (.apkg)** file or **Anki Collection Package (.colpkg)** file on the `Left sidebar -> Files`, drag and drop your file in the current directory (not the `sample_data` directory). \n",
        "\n",
        "No need to include media. Need to include scheduling information. \n",
        "\n",
        "> If you use the latest version of Anki, please check the box `Support older Anki versions (slower/larger files)` when you export.\n",
        "\n",
        "You can export it via `File -> Export...` or `Ctrl + E` in the main window of Anki.\n",
        "\n",
        "Then replace the `filename` with yours in the next code cell. And set the `timezone` and `next_day_starts_at` which can be found in your preferences of Anki.\n",
        "\n",
        "After that, just run all (`Runtime -> Run all` or `Ctrl + F9`) and wait for minutes. You can see the optimal parameters in section **2.3 Result**. Copy them, replace the parameters in `fsrs4anki_scheduler.js`, and paste them into the custom scheduling of your deck options (require Anki version >= 2.1.55).\n",
        "\n",
        "**NOTE**: The default output is generated from my review logs. If you find the output is the same as mine, maybe your notebook hasn't run there.\n",
        "\n",
        "**Contribute to SRS Research**: If you want to share your data with me, please fill this form: https://forms.gle/KaojsBbhMCytaA7h8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqP70_-3EUhi"
      },
      "outputs": [],
      "source": [
        "# Here are some settings that you need to replace before running this optimizer.\n",
        "\n",
        "filename = \"collection-2022-09-18@13-21-58.colpkg\"\n",
        "# If you upload deck file, replace it with your deck filename. E.g., ALL__Learning.apkg\n",
        "# If you upload collection file, replace it with your colpgk filename. E.g., collection-2022-09-18@13-21-58.colpkg\n",
        "\n",
        "# Replace it with your timezone. I'm in China, so I use Asia/Shanghai.\n",
        "# You can find your timezone here: https://gist.github.com/heyalexej/8bf688fd67d7199be4a1682b3eec7568\n",
        "timezone = 'Asia/Shanghai'\n",
        "\n",
        "# Replace it with your Anki's setting in Preferences -> Scheduling.\n",
        "next_day_starts_at = 4\n",
        "\n",
        "# Replace it if you don't want the optimizer to use the review logs before a specific date.\n",
        "revlog_start_date = \"2006-10-05\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLFVNmG2qd06"
      },
      "source": [
        "## 1 Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkzFeKawqgbs"
      },
      "source": [
        "### 1.1 Extract Anki collection & deck file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD2js_wEr_Bs",
        "outputId": "42653d9e-316e-40bc-bd1d-f3a0e2b246c7"
      },
      "outputs": [],
      "source": [
        "import fsrs_optimizer\n",
        "\n",
        "optimizer = fsrs_optimizer.Optimizer()\n",
        "\n",
        "optimizer.anki_extract(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpy4VfqGmaL"
      },
      "source": [
        "### 1.2 Create time-series feature & analysis\n",
        "\n",
        "The following code cell will extract the review logs from your Anki collection and preprocess them to a trainset which is saved in `revlog_history.tsv`.\n",
        "\n",
        "The time-series features are important in optimizing the model's parameters. For more detail, please see my paper: https://www.maimemo.com/paper/\n",
        "\n",
        "Then it will generate a concise analysis for your review logs. \n",
        "\n",
        "- The `r_history` is the history of ratings on each review. `3,3,3,1` means that you press `Good, Good, Good, Again`. It only contains the first rating for each card on the review date, i.e., when you press `Again` in review and  `Good` in relearning steps 10min later, only `Again` will be recorded.\n",
        "- The `avg_interval` is the actual average interval after you rate your cards as the `r_history`. It could be longer than the interval given by Anki's built-in scheduler because you reviewed some overdue cards.\n",
        "- The `avg_retention` is the average retention after you press as the `r_history`. `Again` counts as failed recall, and `Hard, Good and Easy` count as successful recall. Retention is the percentage of your successful recall.\n",
        "- The `stability` is the estimated memory state variable, which is an approximate interval that leads to 90% retention.\n",
        "- The `factor` is `stability / previous stability`.\n",
        "- The `group_cnt` is the number of review logs that have the same `r_history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2IIaY3PDaaG",
        "outputId": "607916c9-da95-48dd-fdab-6bd83fbbbb40"
      },
      "outputs": [],
      "source": [
        "optimizer.create_time_series(timezone, revlog_start_date, next_day_starts_at)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_SgzC-auWmu"
      },
      "source": [
        "## 2 Optimize parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrfBJjqCHEwJ"
      },
      "source": [
        "### 2.1 Define the model\n",
        "\n",
        "FSRS is a time-series model for predicting memory states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdYp3GMLhTYm"
      },
      "outputs": [],
      "source": [
        "optimizer.define_model()\n",
        "print(optimizer.init_w)\n",
        "'''\n",
        "w[0]: initial_stability_for_again_answer\n",
        "w[1]: initial_stability_step_per_rating\n",
        "w[2]: initial_difficulty_for_good_answer\n",
        "w[3]: initial_difficulty_step_per_rating\n",
        "w[4]: next_difficulty_step_per_rating\n",
        "w[5]: next_difficulty_reversion_to_mean_speed (used to avoid ease hell)\n",
        "w[6]: next_stability_factor_after_success\n",
        "w[7]: next_stability_stabilization_decay_after_success\n",
        "w[8]: next_stability_retrievability_gain_after_success\n",
        "w[9]: next_stability_factor_after_failure\n",
        "w[10]: next_stability_difficulty_decay_after_success\n",
        "w[11]: next_stability_stability_gain_after_failure\n",
        "w[12]: next_stability_retrievability_gain_after_failure\n",
        "For more details about the parameters, please see: \n",
        "https://github.com/open-spaced-repetition/fsrs4anki/wiki/Free-Spaced-Repetition-Scheduler\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1dYfgQLZAC"
      },
      "source": [
        "### 2.2 Train the model\n",
        "\n",
        "The `revlog_history.tsv` generated before will be used for training the FSRS model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jht0gneShowU",
        "outputId": "aaa72b79-b454-483b-d746-df1a353b2c8f"
      },
      "outputs": [],
      "source": [
        "optimizer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ4S2l7BWfzr"
      },
      "source": [
        "### 2.3 Result\n",
        "\n",
        "Copy the optimal parameters for FSRS for you in the output of next code cell after running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnPSDA2QpUu",
        "outputId": "49f487b9-69a7-4e96-b35a-7e027f478fbd"
      },
      "outputs": [],
      "source": [
        "print(optimizer.w)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_zsoDyTaTrT"
      },
      "source": [
        "You can see the memory states and intervals generated by FSRS as if you press the good in each review at the due date scheduled by FSRS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iws4rtP1WKBT",
        "outputId": "890d0287-1a17-4c59-fbbf-ee54d79cd383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1:again, 2:hard, 3:good, 4:easy\n",
            "\n",
            "first rating: 1\n",
            "rating history: 1,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,1,1,2,3,4,6,8,11,15,21\n",
            "difficulty history: 0,7.2,7.1,7.0,7.0,6.9,6.9,6.8,6.8,6.7,6.7\n",
            "\n",
            "first rating: 2\n",
            "rating history: 2,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,2,3,4,6,9,13,19,27,37,51\n",
            "difficulty history: 0,6.3,6.2,6.2,6.2,6.1,6.1,6.1,6.1,6.0,6.0\n",
            "\n",
            "first rating: 3\n",
            "rating history: 3,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,3,5,8,12,18,26,38,54,75,103\n",
            "difficulty history: 0,5.3,5.3,5.3,5.3,5.3,5.3,5.3,5.3,5.3,5.3\n",
            "\n",
            "first rating: 4\n",
            "rating history: 4,3,3,3,3,3,3,3,3,3,3\n",
            "interval history: 0,4,7,11,17,26,39,58,83,118,164\n",
            "difficulty history: 0,4.4,4.5,4.5,4.5,4.6,4.6,4.6,4.6,4.6,4.7\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "\n",
        "class Collection:\n",
        "    def __init__(self, w):\n",
        "        self.model = fsrs_optimizer.FSRS(w)\n",
        "\n",
        "    def states(self, t_history, r_history):\n",
        "        with torch.no_grad():\n",
        "            line_tensor = fsrs_optimizer.lineToTensor(list(zip([t_history], [r_history]))[0])\n",
        "            output_t = [(self.model.zero, self.model.zero)]\n",
        "            for input_t in line_tensor:\n",
        "                output_t.append(self.model(input_t, *output_t[-1]))\n",
        "            return output_t[-1]\n",
        "\n",
        "\n",
        "my_collection = Collection(optimizer.w)\n",
        "print(\"1:again, 2:hard, 3:good, 4:easy\\n\")\n",
        "for first_rating in (1,2,3,4):\n",
        "    print(f'first rating: {first_rating}')\n",
        "    t_history = \"0\"\n",
        "    d_history = \"0\"\n",
        "    r_history = f\"{first_rating}\"  # the first rating of the new card\n",
        "    # print(\"stability, difficulty, lapses\")\n",
        "    for i in range(10):\n",
        "        states = my_collection.states(t_history, r_history)\n",
        "        # print('{0:9.2f} {1:11.2f} {2:7.0f}'.format(\n",
        "            # *list(map(lambda x: round(float(x), 4), states))))\n",
        "        next_t = max(round(float(np.log(requestRetention)/np.log(0.9) * states[0])), 1)\n",
        "        difficulty = round(float(states[1]), 1)\n",
        "        t_history += f',{int(next_t)}'\n",
        "        d_history += f',{difficulty}'\n",
        "        r_history += f\",3\"\n",
        "    print(f\"rating history: {r_history}\")\n",
        "    print(f\"interval history: {t_history}\")\n",
        "    print(f\"difficulty history: {d_history}\")\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can change the `test_rating_sequence` to see the scheduling intervals in different ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor(2.8627), tensor(5.3477))\n",
            "(tensor(4.7141), tensor(5.3477))\n",
            "(tensor(7.5727), tensor(5.3477))\n",
            "(tensor(11.8280), tensor(5.3477))\n",
            "(tensor(17.7991), tensor(5.3477))\n",
            "(tensor(3.3788), tensor(6.7091))\n",
            "(tensor(1.0412), tensor(8.0279))\n",
            "(tensor(1.4315), tensor(7.9443))\n",
            "(tensor(1.8163), tensor(7.8633))\n",
            "(tensor(2.5676), tensor(7.7848))\n",
            "(tensor(3.6596), tensor(7.7088))\n",
            "(tensor(5.0732), tensor(7.6351))\n",
            "rating history: 3,3,3,3,3,1,1,3,3,3,3,3\n",
            "interval history: 0,3,5,8,12,18,3,1,1,2,3,4,5\n",
            "difficulty history: 0,5.3,5.3,5.3,5.3,5.3,6.7,8.0,7.9,7.9,7.8,7.7,7.6\n"
          ]
        }
      ],
      "source": [
        "test_rating_sequence = \"3,3,3,3,3,1,1,3,3,3,3,3\"\n",
        "requestRetention = 0.9  # recommended setting: 0.8 ~ 0.9\n",
        "easyBonus = 1.3\n",
        "hardInterval = 1.2\n",
        "\n",
        "t_history = \"0\"\n",
        "d_history = \"0\"\n",
        "for i in range(len(test_rating_sequence.split(','))):\n",
        "    rating = test_rating_sequence[2*i]\n",
        "    last_t = int(t_history.split(',')[-1])\n",
        "    r_history = test_rating_sequence[:2*i+1]\n",
        "    states = my_collection.states(t_history, r_history)\n",
        "    print(states)\n",
        "    next_t = max(1,round(float(np.log(requestRetention)/np.log(0.9) * states[0])))\n",
        "    if rating == '4':\n",
        "        next_t = round(next_t * easyBonus)\n",
        "    elif rating == '2':\n",
        "        next_t = round(last_t * hardInterval)\n",
        "    t_history += f',{int(next_t)}'\n",
        "    difficulty = round(float(states[1]), 1)\n",
        "    d_history += f',{difficulty}'\n",
        "print(f\"rating history: {test_rating_sequence}\")\n",
        "print(f\"interval history: {t_history}\")\n",
        "print(f\"difficulty history: {d_history}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Predict memory states and distribution of difficulty\n",
        "\n",
        "Predict memory states for each review and save them in `prediction.tsv`.\n",
        "\n",
        "Meanwhile, it will count the distribution of difficulty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m     group[\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(group)\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mr_history\u001b[39m\u001b[39m'\u001b[39m: [group\u001b[39m.\u001b[39mname[\u001b[39m1\u001b[39m]], \n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mt_history\u001b[39m\u001b[39m'\u001b[39m: [group\u001b[39m.\u001b[39mname[\u001b[39m0\u001b[39m]], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39mlen\u001b[39m(group)] \n\u001b[1;32m     12\u001b[0m     })\n\u001b[0;32m---> 14\u001b[0m prediction \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mgroupby(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mt_history\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr_history\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mprogress_apply(predict_memory_states)\n\u001b[1;32m     15\u001b[0m prediction\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m prediction\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mr_history\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "def predict_memory_states(group):\n",
        "    states = my_collection.states(*group.name)\n",
        "    group['stability'] = float(states[0])\n",
        "    group['difficulty'] = float(states[1])\n",
        "    group['count'] = len(group)\n",
        "    return pd.DataFrame({\n",
        "        'r_history': [group.name[1]], \n",
        "        't_history': [group.name[0]], \n",
        "        'stability': [round(float(states[0]),2)], \n",
        "        'difficulty': [round(float(states[1]),2)], \n",
        "        'count': [len(group)] \n",
        "    })\n",
        "\n",
        "prediction = optimizer.dataset.groupby(by=['t_history', 'r_history']).progress_apply(predict_memory_states)\n",
        "prediction.reset_index(drop=True, inplace=True)\n",
        "prediction.sort_values(by=['r_history'], inplace=True)\n",
        "prediction.to_csv(\"./prediction.tsv\", sep='\\t', index=None)\n",
        "print(\"prediction.tsv saved.\")\n",
        "prediction['difficulty'] = prediction['difficulty'].map(lambda x: int(round(x)))\n",
        "difficulty_distribution = prediction.groupby(by=['difficulty'])['count'].sum() / prediction['count'].sum()\n",
        "print(difficulty_distribution)\n",
        "difficulty_distribution_padding = np.zeros(10)\n",
        "for i in range(10):\n",
        "    if i+1 in difficulty_distribution.index:\n",
        "        difficulty_distribution_padding[i] = difficulty_distribution.loc[i+1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 Optimize retention to minimize the time of reviews\n",
        "\n",
        "Calculate the optimal retention to minimize the time for long-term memory consolidation. It is an experimental feature. You can use the simulator to get more accurate results:\n",
        "\n",
        "https://github.com/open-spaced-repetition/fsrs4anki/blob/main/fsrs4anki_simulator.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'type_sequence' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m type_count \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     14\u001b[0m type_time \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m---> 15\u001b[0m last_t \u001b[39m=\u001b[39m type_sequence[\u001b[39m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m type_block[last_t] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     17\u001b[0m type_count[last_t] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'type_sequence' is not defined"
          ]
        }
      ],
      "source": [
        "base = 1.01\n",
        "index_len = 793\n",
        "index_offset = 200\n",
        "d_range = 10\n",
        "d_offset = 1\n",
        "r_time = 8\n",
        "f_time = 25\n",
        "max_time = 200000\n",
        "\n",
        "type_block = dict()\n",
        "type_count = dict()\n",
        "type_time = dict()\n",
        "last_t = type_sequence[0]\n",
        "type_block[last_t] = 1\n",
        "type_count[last_t] = 1\n",
        "type_time[last_t] = time_sequence[0]\n",
        "for i,t in enumerate(type_sequence[1:]):\n",
        "    type_count[t] = type_count.setdefault(t, 0) + 1\n",
        "    type_time[t] = type_time.setdefault(t, 0) + time_sequence[i]\n",
        "    if t != last_t:\n",
        "        type_block[t] = type_block.setdefault(t, 0) + 1\n",
        "    last_t = t\n",
        "\n",
        "r_time = round(type_time[1]/type_count[1]/1000, 1)\n",
        "\n",
        "if 2 in type_count and 2 in type_block:\n",
        "    f_time = round(type_time[2]/type_block[2]/1000 + r_time, 1)\n",
        "\n",
        "print(f\"average time for failed cards: {f_time}s\")\n",
        "print(f\"average time for recalled cards: {r_time}s\")\n",
        "\n",
        "def stability2index(stability):\n",
        "    return int(round(np.log(stability) / np.log(base)) + index_offset)\n",
        "\n",
        "def init_stability(d):\n",
        "    return max(((d - w[2]) / w[3] + 2) * w[1] + w[0], np.power(base, -index_offset))\n",
        "\n",
        "def cal_next_recall_stability(s, r, d, response):\n",
        "    if response == 1:\n",
        "        return s * (1 + np.exp(w[6]) * (11 - d) * np.power(s, w[7]) * (np.exp((1 - r) * w[8]) - 1))\n",
        "    else:\n",
        "        return w[9] * np.power(d, w[10]) * np.power(s, w[11]) * np.exp((1 - r) * w[12])\n",
        "\n",
        "\n",
        "stability_list = np.array([np.power(base, i - index_offset) for i in range(index_len)])\n",
        "print(f\"terminal stability: {stability_list.max(): .2f}\")\n",
        "df = pd.DataFrame(columns=[\"retention\", \"difficulty\", \"time\"])\n",
        "\n",
        "for percentage in notebook.tqdm(range(96, 66, -2)):\n",
        "    recall = percentage / 100\n",
        "    time_list = np.zeros((d_range, index_len))\n",
        "    time_list[:,:-1] = max_time\n",
        "    for d in range(d_range, 0, -1):\n",
        "        s0 = init_stability(d)\n",
        "        s0_index = stability2index(s0)\n",
        "        diff = max_time\n",
        "        while diff > 0.1:\n",
        "            s0_time = time_list[d - 1][s0_index]\n",
        "            for s_index in range(index_len - 2, -1, -1):\n",
        "                stability = stability_list[s_index];\n",
        "                interval = max(1, round(stability * np.log(recall) / np.log(0.9)))\n",
        "                p_recall = np.power(0.9, interval / stability)\n",
        "                recall_s = cal_next_recall_stability(stability, p_recall, d, 1)\n",
        "                forget_d = min(d + d_offset, 10)\n",
        "                forget_s = cal_next_recall_stability(stability, p_recall, forget_d, 0)\n",
        "                recall_s_index = min(stability2index(recall_s), index_len - 1)\n",
        "                forget_s_index = min(max(stability2index(forget_s), 0), index_len - 1)\n",
        "                recall_time = time_list[d - 1][recall_s_index] + r_time\n",
        "                forget_time = time_list[forget_d - 1][forget_s_index] + f_time\n",
        "                exp_time = p_recall * recall_time + (1.0 - p_recall) * forget_time\n",
        "                if exp_time < time_list[d - 1][s_index]:\n",
        "                    time_list[d - 1][s_index] = exp_time\n",
        "            diff = s0_time - time_list[d - 1][s0_index]\n",
        "        df.loc[0 if pd.isnull(df.index.max()) else df.index.max() + 1] = [recall, d, s0_time]\n",
        "\n",
        "df.sort_values(by=[\"difficulty\", \"retention\"], inplace=True)\n",
        "df.to_csv(\"./expected_time.csv\", index=False)\n",
        "print(\"expected_time.csv saved.\")\n",
        "\n",
        "optimal_retention_list = np.zeros(10)\n",
        "for d in range(1, d_range+1):\n",
        "    retention = df[df[\"difficulty\"] == d][\"retention\"]\n",
        "    time = df[df[\"difficulty\"] == d][\"time\"]\n",
        "    optimal_retention = retention.iat[time.argmin()]\n",
        "    optimal_retention_list[d-1] = optimal_retention\n",
        "    plt.plot(retention, time, label=f\"d={d}, r={optimal_retention}\")\n",
        "print(f\"\\n-----suggested retention (experimental): {np.inner(difficulty_distribution_padding, optimal_retention_list):.2f}-----\")\n",
        "plt.ylabel(\"expected time (second)\")\n",
        "plt.xlabel(\"retention\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.semilogy()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Evaluate the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Loss\n",
        "\n",
        "Evaluate the model with the log loss. It will compare the log loss between initial model and trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_collection = Collection(init_w)\n",
        "optimizer.dataset['stability'] = optimizer.dataset.progress_apply(lambda row: my_collection.states(row['t_history'], row['r_history'])[0].item(), axis=1)\n",
        "optimizer.dataset['p'] = np.exp(np.log(0.9) * optimizer.dataset['delta_t'] / optimizer.dataset['stability'])\n",
        "optimizer.dataset['log_loss'] = optimizer.dataset.apply(lambda row: - np.log(row['p']) if row['y'] == 1 else - np.log(1 - row['p']), axis=1)\n",
        "print(f\"Loss before training: {optimizer.dataset['log_loss'].mean():.4f}\")\n",
        "\n",
        "my_collection = Collection(w)\n",
        "optimizer.dataset['stability'] = optimizer.dataset.progress_apply(lambda row: my_collection.states(row['t_history'], row['r_history'])[0].item(), axis=1)\n",
        "optimizer.dataset['p'] = np.exp(np.log(0.9) * optimizer.dataset['delta_t'] / optimizer.dataset['stability'])\n",
        "optimizer.dataset['log_loss'] = optimizer.dataset.apply(lambda row: - np.log(row['p']) if row['y'] == 1 else - np.log(1 - row['p']), axis=1)\n",
        "print(f\"Loss after training: {optimizer.dataset['log_loss'].mean():.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Calibration graph\n",
        "\n",
        "1. FSRS predicts the stability and retention for each review.\n",
        "2. Reviews are grouped into 40 bins according to their predicted retention.\n",
        "3. Count the true retention of each bin.\n",
        "4. Plot (predicted retention, true retention) in the line graph.\n",
        "5. Plot (predicted retention, size of bin) in the bar graph.\n",
        "6. Combine these graphs to create the calibration graph.\n",
        "\n",
        "Ideally, the blue line should be aligned with the orange one. If the blue line is higher than the orange line, the FSRS underestimates the retention. When the size of reviews within a bin is small, actual retention may deviate largely, which is normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def r_squared(predictions, real, weights):\n",
        "    x = np.array(predictions)\n",
        "    y = np.array(real)\n",
        "    weights = np.array(weights)\n",
        "    weighted_mean_x = np.average(x, weights=weights)\n",
        "    weighted_mean_y = np.average(y, weights=weights)\n",
        "    cov_xy = np.sum(weights * (x - weighted_mean_x) * (y - weighted_mean_y)) / np.sum(weights)\n",
        "    var_x = np.sum(weights * (x - weighted_mean_x)**2) / np.sum(weights)\n",
        "    var_y = np.sum(weights * (y - weighted_mean_y)**2) / np.sum(weights)\n",
        "    r = cov_xy / np.sqrt(var_x * var_y)\n",
        "    r_squared = r**2\n",
        "    print(f\"R^2: {r_squared:.4f}\")\n",
        "\n",
        "\n",
        "# code from https://github.com/papousek/duolingo-halflife-regression/blob/master/evaluation.py\n",
        "def load_brier(predictions, real, bins=20):\n",
        "    counts = np.zeros(bins)\n",
        "    correct = np.zeros(bins)\n",
        "    prediction = np.zeros(bins)\n",
        "    for p, r in zip(predictions, real):\n",
        "        bin = min(int(p * bins), bins - 1)\n",
        "        counts[bin] += 1\n",
        "        correct[bin] += r\n",
        "        prediction[bin] += p\n",
        "    prediction_means = prediction / counts\n",
        "    prediction_means[np.isnan(prediction_means)] = ((np.arange(bins) + 0.5) / bins)[np.isnan(prediction_means)]\n",
        "    correct_means = correct / counts\n",
        "    correct_means[np.isnan(correct_means)] = 0\n",
        "    size = len(predictions)\n",
        "    answer_mean = sum(correct) / size\n",
        "    return {\n",
        "        \"reliability\": sum(counts * (correct_means - prediction_means) ** 2) / size,\n",
        "        \"resolution\": sum(counts * (correct_means - answer_mean) ** 2) / size,\n",
        "        \"uncertainty\": answer_mean * (1 - answer_mean),\n",
        "        \"detail\": {\n",
        "            \"bin_count\": bins,\n",
        "            \"bin_counts\": list(counts),\n",
        "            \"bin_prediction_means\": list(prediction_means),\n",
        "            \"bin_correct_means\": list(correct_means),\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_brier(predictions, real, bins=20):\n",
        "    brier = load_brier(predictions, real, bins=bins)\n",
        "    bin_prediction_means = brier['detail']['bin_prediction_means']\n",
        "    bin_correct_means = brier['detail']['bin_correct_means']\n",
        "    bin_counts = brier['detail']['bin_counts']\n",
        "    r_squared(bin_prediction_means, bin_correct_means, bin_counts)\n",
        "    plt.figure()\n",
        "    plt.plot(bin_prediction_means, bin_correct_means, label='Average actual retention')\n",
        "    plt.plot((0, 1), (0, 1), label='Optimal average actual retention')\n",
        "    bin_count = brier['detail']['bin_count']\n",
        "    counts = np.array(bin_counts)\n",
        "    bins = (np.arange(bin_count) + 0.5) / bin_count\n",
        "    plt.legend(loc='upper center')\n",
        "    plt.xlabel('Predicted Retention')\n",
        "    plt.ylabel('Actual Retention')\n",
        "    plt.twinx()\n",
        "    plt.ylabel('Number of predictions')\n",
        "    plt.bar(bins, counts, width=(0.5 / bin_count), alpha=0.5, label='Number of predictions')\n",
        "    plt.legend(loc='lower center')\n",
        "\n",
        "\n",
        "plot_brier(dataset['p'], dataset['y'], bins=40)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMnk8/Ih2JAJZJ1PBkXQUBC",
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "46e27ede752268be201d36b7fbc2802b29a11b0bb095abacecc6c0428b93624a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
